#! /usr/bin/env python

import traceback
import sys
import os
import stat
import errno
import tempfile
import pexpect
import time
import re
from datetime import datetime, timedelta

# pull in some spaghetti to make this stuff work without fuse-py being installed
try:
    import _find_fuse_parts
except ImportError:
    pass

import fuse
from fuse import Fuse

if not hasattr(fuse, '__version__'):
    raise RuntimeError, \
        "your fuse-py doesn't know of fuse.__version__, probably it's too old."

fuse.fuse_python_api = (0, 2)

fuse.feature_assert('stateful_files', 'has_init')

__version__ = '0.0.1'


def flag2mode(flags):
    '''
    taken from python-fuse xmp.py example
    '''
    md = {os.O_RDONLY: 'r', os.O_WRONLY: 'w', os.O_RDWR: 'w+'}
    m = md[flags & (os.O_RDONLY | os.O_WRONLY | os.O_RDWR)]
    
    if flags | os.O_APPEND:
        m = m.replace('w', 'a', 1)
        
    return m

def makedirs(path):
    '''
    create path like mkdir -p
    taken from: http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python/600612#600612
    '''
    try:
        os.makedirs(path)
    except OSError, exc:
        if exc.errno == errno.EEXIST:
            pass
        else:
            raise
                            
def touch(fname, times = None):
    '''
    touch file
    adapted from: http://stackoverflow.com/questions/1158076/implement-touch-using-python/1160227#1160227
    '''
    fhandle = open(fname, 'a')
    try:
        os.utime(fname, times)
    finally:
        fhandle.close()
        

def bextract_version() :
    '''
    return version string of bextract,
    return None if not runnable or version cannot be parsed
    '''
    version = None
    try :
        child = pexpect.spawn('bextract -?')
        i = child.expect(['Version: ([^(]*) \(([^)]*)\)', pexpect.EOF])
        if i == 0 :
            version = '%s (%s)' % child.match.groups()
        child.close()
    except :
        pass
    return version

class SQL :
    '''
    Holds all SQL statements used by baculafs.
    Adapted from Bacula source code.
    '''

    clients = 'SELECT Client.Name,ClientId FROM Client'
    
    filesets = '''
    SELECT DISTINCT FileSet.FileSet FROM Job,
    Client,FileSet WHERE Job.FileSetId=FileSet.FileSetId
    AND Job.ClientId=%s AND Client.ClientId=%s 
    ORDER BY FileSet.FileSet
    '''

    fileset = '''
    SELECT FileSetId,FileSet,MD5,CreateTime FROM FileSet
    WHERE FileSet="%s" ORDER BY CreateTime DESC LIMIT 1
    '''

    create_temp = '''
    CREATE TEMPORARY TABLE temp (
    JobId INTEGER UNSIGNED NOT NULL,
    JobTDate BIGINT UNSIGNED,
    ClientId INTEGER UNSIGNED,
    Level CHAR,
    JobFiles INTEGER UNSIGNED,
    JobBytes BIGINT UNSIGNED,
    StartTime TEXT,
    VolumeName TEXT,
    StartFile INTEGER UNSIGNED,
    VolSessionId INTEGER UNSIGNED,
    VolSessionTime INTEGER UNSIGNED)
    '''

    create_temp1 = '''
    CREATE TEMPORARY TABLE temp1 (
    JobId INTEGER UNSIGNED NOT NULL,
    JobTDate BIGINT UNSIGNED)
    '''

    temp  = 'SELECT * FROM temp'

    temp1 = 'SELECT * FROM temp1'

    del_temp = 'DROP TABLE temp'
    
    del_temp1 = 'DROP TABLE temp1'

    full_jobs_temp1 = '''
    INSERT INTO temp1 SELECT Job.JobId,JobTdate 
    FROM Client,Job,JobMedia,Media,FileSet WHERE Client.ClientId=%s
    AND Job.ClientId=%s
    AND Job.StartTime < DATETIME("%s")
    AND Level='F' AND JobStatus IN ('T','W') AND Type='B' 
    AND JobMedia.JobId=Job.JobId 
    AND Media.Enabled=1 
    AND JobMedia.MediaId=Media.MediaId 
    AND Job.FileSetId=FileSet.FileSetId 
    AND FileSet.FileSet="%s"
    ORDER BY Job.JobTDate DESC LIMIT 1
    '''

    full_jobs_temp = '''
    INSERT INTO temp SELECT Job.JobId,Job.JobTDate,
    Job.ClientId,Job.Level,Job.JobFiles,Job.JobBytes,
    StartTime,VolumeName,JobMedia.StartFile,VolSessionId,VolSessionTime 
    FROM temp1,Job,JobMedia,Media WHERE temp1.JobId=Job.JobId 
    AND Level='F' AND JobStatus IN ('T','W') AND Type='B' 
    AND Media.Enabled=1 
    AND JobMedia.JobId=Job.JobId 
    AND JobMedia.MediaId=Media.MediaId
    '''

    diff_jobs_temp = '''
    INSERT INTO temp SELECT Job.JobId,Job.JobTDate,Job.ClientId,
    Job.Level,Job.JobFiles,Job.JobBytes,
    Job.StartTime,Media.VolumeName,JobMedia.StartFile,
    Job.VolSessionId,Job.VolSessionTime 
    FROM Job,JobMedia,Media,FileSet 
    WHERE Job.JobTDate>%d AND Job.StartTime<DATETIME("%s")
    AND Job.ClientId=%d 
    AND JobMedia.JobId=Job.JobId 
    AND Media.Enabled=1 
    AND JobMedia.MediaId=Media.MediaId 
    AND Job.Level='D' AND JobStatus IN ('T','W') AND Type='B' 
    AND Job.FileSetId=FileSet.FileSetId 
    AND FileSet.FileSet="%s"
    ORDER BY Job.JobTDate DESC LIMIT 1
    '''

    incr_jobs_temp = '''
    INSERT INTO temp SELECT Job.JobId,Job.JobTDate,Job.ClientId,
    Job.Level,Job.JobFiles,Job.JobBytes,
    Job.StartTime,Media.VolumeName,JobMedia.StartFile,
    Job.VolSessionId,Job.VolSessionTime 
    FROM Job,JobMedia,Media,FileSet 
    WHERE Job.JobTDate>%d AND Job.StartTime<DATETIME("%s") 
    AND Job.ClientId=%d
    AND Media.Enabled=1 
    AND JobMedia.JobId=Job.JobId 
    AND JobMedia.MediaId=Media.MediaId 
    AND Job.Level='I' AND JobStatus IN ('T','W') AND Type='B' 
    AND Job.FileSetId=FileSet.FileSetId 
    AND FileSet.FileSet="%s"
    '''

    jobs = 'SELECT DISTINCT JobId,StartTime FROM temp ORDER BY StartTime ASC'

    base_jobs = '''
    SELECT DISTINCT BaseJobId
    FROM Job JOIN BaseFiles USING (JobId)
    WHERE Job.HasBase = 1
    AND Job.JobId IN (%s)
    '''

    purged_jobs = '''
    SELECT SUM(PurgedFiles) FROM Job WHERE JobId IN (%s)
    '''
    
    files = '''
    SELECT Path.Path, Filename.Name, Temp.FileIndex, Temp.JobId, LStat, MD5 
     FROM ( %s ) AS Temp 
     JOIN Filename ON (Filename.FilenameId = Temp.FilenameId) 
     JOIN Path ON (Path.PathId = Temp.PathId) 
    WHERE FileIndex > 0 
    ORDER BY Temp.JobId, FileIndex ASC
    '''

    with_basejobs = '''
    SELECT FileId, Job.JobId AS JobId, FileIndex, File.PathId AS PathId, 
           File.FilenameId AS FilenameId, LStat, MD5 
    FROM Job, File, ( 
        SELECT MAX(JobTDate) AS JobTDate, PathId, FilenameId 
          FROM ( 
            SELECT JobTDate, PathId, FilenameId 
              FROM File JOIN Job USING (JobId) 
             WHERE File.JobId IN (%s) 
              UNION ALL 
            SELECT JobTDate, PathId, FilenameId 
              FROM BaseFiles 
                   JOIN File USING (FileId) 
                   JOIN Job  ON    (BaseJobId = Job.JobId) 
             WHERE BaseFiles.JobId IN (%s) 
           ) AS tmp GROUP BY PathId, FilenameId 
        ) AS T1 
    WHERE (Job.JobId IN ( 
             SELECT DISTINCT BaseJobId FROM BaseFiles WHERE JobId IN (%s)) 
            OR Job.JobId IN (%s)) 
      AND T1.JobTDate = Job.JobTDate 
      AND Job.JobId = File.JobId 
      AND T1.PathId = File.PathId 
      AND T1.FilenameId = File.FilenameId
    '''

    job_records = '''
    SELECT JobId,VolSessionId,VolSessionTime,
    PoolId,StartTime,EndTime,JobFiles,JobBytes,JobTDate,Job,JobStatus,
    Type,Level,ClientId,Name,PriorJobId,RealEndTime,FileSetId,
    SchedTime,RealEndTime,ReadBytes,HasBase 
    FROM Job WHERE JobId IN (%s)
    '''

    volumes = '''
    SELECT JobMedia.JobId,VolumeName,MediaType,FirstIndex,LastIndex,StartFile,
    JobMedia.EndFile,StartBlock,JobMedia.EndBlock,Copy,
    Slot,StorageId,InChanger
     FROM JobMedia,Media WHERE JobMedia.JobId IN (%s)
     AND JobMedia.MediaId=Media.MediaId ORDER BY JobMedia.JobId,VolIndex,JobMediaId
    '''
    
class Base64 :
    '''
    Bacula specific implementation of a base64 decoder
    '''
    digits = [
        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',
        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',
        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '/'
        ]

    def __init__(self) :
        '''
        Initialize the Base 64 conversion routines
        '''
        self.base64_map = dict(zip(Base64.digits,xrange(0,64)))
    
    def decode(self, base64) :
        '''
        Convert the Base 64 characters in base64 to a value.
        '''
        value = 0
        first = 0
        neg = False

        if base64[0] == '-' :
            neg = True
            first = 1
            
        for i in xrange(first, len(base64)) :
            value = value << 6
            value += self.base64_map[base64[i]]

        return -value if neg else value
    
class Database :
    '''
    This class shields the rest of the code from the pesky details of
    actually accessing one of the supported databases.
    '''
    def __init__(self, driver, database, username, password, trace) :
        '''
        Initialize database driver: connect the database,
        create connection and cusror objects.
        '''
        self.trace = trace
        self.connection = None
        self.cursor = None
        if driver == 'sqlite3' :
            from sqlite3 import connect
            database = os.path.expanduser(database)
            if not os.path.isfile(database) or not os.access(database, os.R_OK) :
                raise RuntimeError, 'cannot read from file %s' % database
            self.connection = connect(database)
            self.connection.text_factory = str # fixes sqlite3.OperationalError: Could not decode to UTF-8
            self.cursor = self.connection.cursor()
        else :
            raise ValueError, 'driver %s not supported yet.' % driver

    def __del__(self) :
        '''
        Close database connection
        '''
        if self.cursor :
            self.cursor.close()
        if self.connection :
            self.connection.close()

    def query(self, sql) :
        '''
        Execute SQL and fetch all results
        '''
        if self.trace :
            print sql
        self.cursor.execute(sql)
        return self.cursor.fetchall()
        
    
    
class Catalog :
    '''
    This class represents the Bacula catalog, and provides an interface
    for generating a list of files for a given set of user supplied
    query parameters.
    '''
    def __init__(self, database) :
        '''
        Catalog initialization: DATABASE is a Database driver
        object.
        '''
        self.db = database

        
    def query(self, client, fileset = None, timespec = None, select_recent_job = False ) :
        '''
        Query bacula database, get list of files that match
        backup prior to given TIMESPEC, for a given CLIENT, FILESET.

        If the date/time is not specified (i.e. it's None) then use
        the current date/time.

        File records include file path, file name, and stat info.

        Security note: query parameters are never taken from user supplied
        input, but rather are verified against the catalog. This allows us to
        use formatted strings for building parametrized queries.
        '''
        # validate client
        self.clients = dict(self.db.query(SQL.clients))
        if client not in self.clients :
            raise ValueError, 'client must be one of %s' % self.clients.keys()
        self.client_id = self.clients[client]
        # validate fileset
        self.filesets = [f[0] for f in self.db.query(SQL.filesets % (self.client_id, self.client_id))]
        if len(self.filesets) == 1 and not fileset :
            fileset = self.filesets[0]
        elif len(self.filesets) == 0 :
            raise RuntimeError, 'no filesets found for %s' % client
        elif fileset not in self.filesets :
            raise ValueError, 'fileset must be one of %s' % self.filesets
        self.fileset = self.db.query(SQL.fileset % fileset)[0]
        # validate timespec
        if timespec :
            self.datetime = datetime.isoformat(datetime.strptime(timespec, '%Y-%m-%d %H:%M:%S'))
        else :
            self.datetime = str(datetime.now())
        # create temporary tables
        self.db.query(SQL.create_temp)
        self.db.query(SQL.create_temp1)
        # get list of jobs
        self.db.query(SQL.full_jobs_temp1 %
                      (self.client_id, self.client_id, self.datetime, self.fileset[1]))
        self.db.query(SQL.full_jobs_temp)
        full_jobs = self.db.query(SQL.temp1)
        if len(full_jobs) == 0 :
            raise RuntimeError, 'no full jobs found'
        self.db.query(SQL.diff_jobs_temp % (full_jobs[0][1], self.datetime, self.client_id, self.fileset[1]))
        diff_jobs = self.db.query(SQL.temp)
        self.db.query(SQL.incr_jobs_temp % (diff_jobs[-1][1], self.datetime, self.client_id, self.fileset[1]))
        jobs = self.db.query(SQL.jobs)
        self.most_recent_jobid = jobs[-1][0] if len(jobs) > 1 else -1
        # this is a wasteful hack to select files from the most recent job only
        if select_recent_job and self.most_recent_jobid > 0 :
            jobs = [jobs[-1]]
        jobs_csl = ','.join([str(job[0]) for job in jobs])
        base_jobs = self.db.query(SQL.base_jobs % jobs_csl)
        all_jobs = jobs + base_jobs
        all_jobs_csl = ','.join([str(job[0]) for job in all_jobs])
        # abort if any job in the list has been purged
        purged = self.db.query(SQL.purged_jobs % all_jobs_csl)
        if purged[0][0] > 0 :
            raise RuntimeError, 'purged jobs in list (%s)' % all_jobs_csl
        # get job records
        self.jobs = self.db.query(SQL.job_records % all_jobs_csl)
        # get relevant volume records
        self.volumes = self.db.query(SQL.volumes % all_jobs_csl)
        # get files
        self.files = self.db.query(SQL.files % (SQL.with_basejobs % (jobs_csl, jobs_csl, jobs_csl, jobs_csl)))
        # delete temporary tables
        self.db.query(SQL.del_temp)
        self.db.query(SQL.del_temp1)

        return self.files


class FileSystem(Fuse) :

    null_stat = fuse.Stat(st_mode = stat.S_IFDIR | 0755, st_nlink = 2)

    bacula_stat_fields = ['st_dev',
                          'st_ino',
                          'st_mode',
                          'st_nlink',
                          'st_uid',
                          'st_gid',
                          'st_rdev',
                          'st_size',
                          'st_blksize',
                          'st_blocks',
                          'st_atime',
                          'st_mtime',
                          'st_ctime',
                          'st_linkfi',
                          'st_flags',
                          'st_streamid']

    fuse_stat_fields = dir(fuse.Stat())

    xattr_fields = ['FileIndex', 'JobId', 'LStat', 'MD5']

    def __init__(self, *args, **kw):
        '''
        Initialize filesystem
        '''

        # default option values
        self.trace = False
        self.driver = 'sqlite3'
        self.database = '/var/lib/bacula/bacula.db'
        self.address = 'localhost'
        self.port = 0
        self.username = 'bacula'
        self.password = ''
        self.conf = '/etc/bacula/bacula-sd.conf'
        self.client = ''
        self.fileset = None
        self.device = 'FileStorage'
        self.retry_interval = 5*60
        self.datetime = None
        self.recent_job = False
        self.user_cache_path = None
        self.move_root = False
        self.notify = False
        self.pynotify = None
        self.notification = None
        self.prefetch_attrs = False
        self.prefetch_regex = None
        self.prefetch_symlinks = False
        self.prefetch_recent = False
        self.prefetch_everything = False
        self.dirs = { '/': { '': (FileSystem.null_stat,) } }

        class File (FileSystem._File):
            def __init__(self2, *a, **kw):
                FileSystem._File.__init__(self2, self, *a, **kw)
                
        self.file_class = File

        Fuse.__init__(self, *args, **kw)


    def _bacula_stat(self, base64) :
        '''
        Parse base64 encoded lstat info.
        Returns fuse.Stat object with subset of decoded values,
        and dictionary with full list of decoded values
        '''
        st = fuse.Stat()
        lst = dict(zip(FileSystem.bacula_stat_fields, map(self.base64.decode, base64.split())))
        for k in FileSystem.bacula_stat_fields :
            if k in FileSystem.fuse_stat_fields :
                setattr(st, k, lst[k])
        return lst, st

    def _notify(self, message, timeout = -1, urgent = False) :
        '''
        Notify message to user
        '''
        if not self.notify :
            print message
            sys.stdout.flush()
            return

        if not self.notification :
            try :
                import pygtk
                pygtk.require('2.0')
                import pynotify
                self.pynotify = pynotify
            except :
                self.notify = False
                print message
                return
            self.pynotify.init("BaculaFS")
            self.notification = self.pynotify.Notification("BaculaFS", message)
        else :
            self.notification.update("BaculaFS", message)

        self.notification.set_timeout(timeout if timeout >= 0 else self.pynotify.EXPIRES_DEFAULT)
        self.notification.set_urgency(self.pynotify.URGENCY_CRITICAL if urgent else self.pynotify.URGENCY_NORMAL)
            
        if not self.notification.show() :
            self.notify = False
            self.notification = None

    def _add_parent_dirs(self, path) :
        '''
        add parent directories of path to dirs dictionary
        '''
        head, tail = os.path.split(path[:-1])
        if not head or head == path:
            return
        head_dir = head if head.endswith('/') else head+'/'
        if not head_dir in self.dirs :
            self.dirs[head_dir] = { tail: (FileSystem.null_stat,) }
        elif not tail in self.dirs[head_dir] :
            self.dirs[head_dir][tail] = (FileSystem.null_stat,)
        self._add_parent_dirs(head_dir)

    
    def _extract(self, path_list) :
        '''
        extract path list from storage, returns path list of extracted files
        '''
        items = []
        realpath_list = []
        
        for path in path_list :
            realpath, symlinkinfo, volumes = self._find_volumes(path)
            realpath_list.append(realpath)
            if volumes :
                items.append((symlinkinfo, volumes))

        if len(items) > 0 :
            rc, sig = self._bextract(items)
            # it seems that bextract does not restore mtime for symlinks
            # so we create a normal file with same mtime as stored symlink
            # (note that we only use that file if the cache path was
            # supplied bu the user)
            if rc == 0 :
                for item in items :
                    if item[0] :
                        symlinkfile = item[0][0]
                        symlinktime = item[0][1:]
                        makedirs(os.path.dirname(symlinkfile))
                        touch(symlinkfile, symlinktime)

        return realpath_list
    

    def _find_volumes(self, path) :
        '''
        return list of volumes that contain path to be extracted, 
        if the path has not been extracted yet
        '''
        realpath = os.path.normpath(self.cache_path + path)
        symlinkpath = os.path.normpath(self.cache_symlinks + path)
        head, tail = os.path.split(path)
        # sanity check: path should not be a directory
        if head == tail or tail == '':
            raise RuntimeError, 'trying to extract a directory %s' % path
        # check that path exists in catalog
        if not head.endswith('/') :
            head = head + '/'
        if head not in self.dirs or tail not in self.dirs[head] :
            return None, None, None
        # sanity check: path entry is incomplete
        if len(self.dirs[head][tail]) == 1 :
            raise RuntimeError, 'incomplete entry for path %s' % path
        # return if file has already been extracted
        bs = self.dirs[head][tail][-1]
        is_symlink = stat.S_ISLNK(bs.st_mode)
        found = False
        if os.path.exists(realpath) or os.path.lexists(realpath) :
            # do not trust user supplied cache path:
            # make sure that stat info of realpath matches path
            if self.user_cache_path :
                s = os.lstat(realpath)
                conds = [getattr(s, attr) == getattr(bs, attr)
                         for attr in ['st_mode', 'st_uid', 'st_gid', 'st_size', 'st_mtime']]
                if is_symlink :
                    conds[-1] = (os.path.exists(symlinkpath) and
                                 bs.st_mtime == os.stat(symlinkpath).st_mtime)
                found = all(conds)
            else :
                found = True
            if found :
                return realpath, None, None
        # generate list of volumes for path
        fileindex, jobid = self.dirs[head][tail][0:2]
        jobs = [job for job in self.catalog.jobs
                if job[0] == jobid]
        volumes = [[volume[1],   # 0-Volume
                    volume[2],   # 1-MediaType
                    self.device, # 2-Device
                    jobs[0][1],  # 3-VolSessionId
                    jobs[0][2],  # 4-VolSessionTime
                    (volume[5] << 32) | volume[7], # 5-VolAddr: StartAddr
                    (volume[6] << 32) | volume[8], # 6-VolAddr: EndAddr
                    fileindex]   # 7-FileIndex
                   for volume in self.catalog.volumes
                   if (volume[0] == jobid and 
                       volume[3] <= fileindex and
                       fileindex <= volume[4])]
        
        return realpath, (symlinkpath, bs.st_atime, bs.st_mtime) if is_symlink else None, volumes
        
    def _bextract(self, items) :
        '''
        extract list of items from Bacula storage device
        '''
        bsrpath = self._write_bsr(items)
        cmd = 'bextract -b "%s" -c "%s" "%s" "%s"' % (bsrpath, self.conf, self.device, self.cache_path)
        if self.trace :
            print cmd
        child = pexpect.spawn(cmd)
        if self.trace :
            child.logfile = sys.stdout

        attempt = 0
        missing = ''
        while True :
            # bextract either finishes or waits for a missing volume
            i = child.expect([self.fail_pattern, pexpect.EOF],
                             timeout=None,
                             searchwindowsize=200)
            if i == 0 :
                # count retries
                if missing == child.match.groups()[0] :
                    attempt += 1
                else :
                    attempt = 1
                    missing = child.match.groups()[0]
                # notify user
                self._notify('\nPlease mount Volume "%s" on device "%s" %s\nRetry #%d scheduled for %s ...\n' %
                             (missing,
                              self.device,
                              child.match.groups()[1],
                              attempt,
                              (datetime.now() + timedelta(seconds = self.retry_interval)).strftime('%F %T')),
                             urgent = True,
                             timeout = self.retry_interval)
                # wait and retry
                time.sleep(self.retry_interval)
                child.sendline('')
            else :
                child.close()
                break

        return (child.exitstatus, child.signalstatus)

    def _write_bsr(self, items) :
        '''
        generate bsr for items to be extracted
        '''
        bsrfd, bsrpath = tempfile.mkstemp(suffix='.bsr', dir=self.cache_bsrpath, text=True)
        for item in items :
            for volume in item[-1] :
                os.write(bsrfd, 'Volume="%s"\n' % volume[0])
                os.write(bsrfd, 'MediaType="%s"\n' % volume[1])
                os.write(bsrfd, 'Device="%s"\n' % volume[2]) 
                os.write(bsrfd, 'VolSessionId=%d\n' % volume[3])
                os.write(bsrfd, 'VolSessionTime=%d\n' % volume[4])
                os.write(bsrfd, 'VolAddr=%d-%d\n' % (volume[5],volume[6]))
                os.write(bsrfd, 'FileIndex=%d\n' % volume[7])
                os.write(bsrfd, 'Count=1\n')
        os.close(bsrfd)
        return bsrpath
        

    def initialize(self):
        '''
        initialize database, catalog
        '''
        print "Populating file system ... "
        if self.user_cache_path :
            self.cache_prefix = self.user_cache_path
        else :
            self.cache_prefix = tempfile.mkdtemp(prefix='baculafs-')
        self.cache_path = os.path.normpath(self.cache_prefix + '/files')
        makedirs(self.cache_path)
        self.cache_bsrpath = os.path.normpath(self.cache_prefix + '/bsr')
        makedirs(self.cache_bsrpath)
        self.cache_symlinks = os.path.normpath(self.cache_prefix + '/symlinks')
        makedirs(self.cache_symlinks)
        self.fail_pattern = 'Mount Volume "([^"]+)" on device "%s" (.*) and press return when ready:' % self.device
        self.db = Database(self.driver,
                           self.database,
                           self.username,
                           self.password,
                           self.trace)
        self.catalog = Catalog(self.db)
        self.base64 = Base64()
        files = self.catalog.query(self.client, self.fileset, self.datetime, self.recent_job)
        # we don't need the database anymore
        self.db = None 

        prefetches = []
        if self.prefetch_everything :
            self.prefetch_recent = False
            self.prefetch_regex = None
            self.prefetch_symlinks = True
        if self.prefetch_regex :
            try :
                regex = re.compile(self.prefetch_regex)
                self.prefetch_attrs = True
            except :
                # bad regex: show traceback and ignore
                traceback.print_exc()
                self.prefetch_regex = None
        if self.prefetch_recent :
            self.prefetch_symlinks = True
        if self.prefetch_symlinks :
            self.prefetch_attrs = True

        for file in files :
            head = file[0]
            tail = file[1]
            # handle windows directories
            if not head.startswith('/') :
                head = '/'+head
            # make file entry
            if self.prefetch_attrs :
                entry = file[2:] + self._bacula_stat(file[-2])
                filepath = head + tail
                if (self.prefetch_everything and 
                    not stat.S_ISDIR(entry[-1].st_mode)) :
                    prefetches.append(filepath)
                elif (self.prefetch_recent and
                      file[3] == self.catalog.most_recent_jobid and
                      not stat.S_ISDIR(entry[-1].st_mode)) :
                    prefetches.append(filepath)
                elif (self.prefetch_regex and
                      not stat.S_ISDIR(entry[-1].st_mode) and
                      regex.match(filepath)) :
                    prefetches.append(filepath)
                elif (self.prefetch_symlinks and
                      stat.S_ISLNK(entry[-1].st_mode)) :
                    prefetches.append(filepath)
            else :
                entry = file[2:] + (None,) # stat info placeholder
            # new directory
            if head not in self.dirs :
                self.dirs[head] = {}
            # add parent directories
            self._add_parent_dirs(head)
            # directories are added to their parents
            if tail == '' :
                head, tail = os.path.split(head[:-1])
                if not head.endswith('/') :
                    head += '/'
            # and finally
            self.dirs[head][tail] = entry

        npf = len(prefetches)
        if npf > 0 :
            print "Prefetching %d objects ... " % npf
            self._extract(prefetches)
        print 'Cache directory is: %s' % self.cache_prefix
        print "BaculaFS ready (%d files)." % len(files)

    def getxattr(self, path, name, size):
        head, tail = os.path.split(path)
        head = head if head.endswith('/') else head+'/'
        if head in self.dirs and tail in self.dirs[head] :
            if len(self.dirs[head][tail]) == 1 :
                return -errno.ENODATA
            else :
                n = name.replace('user.baculafs.', '')
                if n in FileSystem.xattr_fields :
                    val = str(self.dirs[head][tail][FileSystem.xattr_fields.index(n)])
                else :
                    return -errno.ENODATA
                if size == 0:
                    # We are asked for size of the value.
                    return len(val)
                return val
        else :
            return -errno.ENOENT

    def listxattr(self, path, size):
        head, tail = os.path.split(path)
        head = head if head.endswith('/') else head+'/'
        if head in self.dirs and tail in self.dirs[head] :
            if len(self.dirs[head][tail]) == 1 :
                return -errno.ENODATA
            else :
                xattrs = ['user.baculafs.' + a for a in FileSystem.xattr_fields]
            if size == 0:
                # We are asked for size of the attr list, ie. joint size of attrs
                # plus null separators.
                return len("".join(xattrs)) + len(xattrs)
            return xattrs
        else:
            return -errno.ENOENT

    def getattr(self, path):
        '''
        Retrieve file attributes.
        Notes:
        1) Bacula does not store attributes for parent directories
           that are not being explicitly backed up, so we provide
           a default set of attributes FileSystem.null_stat
        2) file attributes are base64-encoded and stored by Bacula
           in the catalog. These attributes are decoded when first
           needed and then cached for subsequent requests.
        3) python fuse expects atime/ctime/mtime to be positive
        '''
        head, tail = os.path.split(path)
        head = head if head.endswith('/') else head+'/'
        if head in self.dirs and tail in self.dirs[head] :
            attrs = self.dirs[head][tail][-1]
            # decode and cache stat info
            if not attrs :
                self.dirs[head][tail] = self.dirs[head][tail][:-1] + self._bacula_stat(self.dirs[head][tail][-3])
                attrs = self.dirs[head][tail][-1]
            # zero negative timestamps
            for a in ['st_atime','st_mtime','st_ctime'] :
                t = getattr(attrs, a) 
                if t < 0 :
                    if self.trace :
                        print 'warning: %s has negative timestamp %s=%d, will use 0' % (path, a, t)
                    setattr(attrs, a, 0)
            return attrs
        else:
            return -errno.ENOENT
    
    def readdir(self, path, offset):
        path = path if path.endswith('/') else path+'/'
        for key in ['.','..']+self.dirs[path].keys() :
            if len(key) > 0:
                yield fuse.Direntry(key)
            
    def readlink(self, path):
        realpath = self._extract([path])[0]
        if realpath :
            link = os.readlink(realpath)
            if self.move_root and link.startswith('/') :
                link = os.path.normpath(self.fuse_args.mountpoint + link)
            return link
        return -errno.ENOENT

    class _File(object) :
        def __init__(self, fs, path, flags, *mode) :
            self.fs = fs
            accmode = os.O_RDONLY | os.O_WRONLY | os.O_RDWR
            if (flags & accmode) != os.O_RDONLY:
                raise IOError(errno.EACCES, '')
            self.path = path
            self.realpath = fs._extract([path])[0]
            self.file = os.fdopen(os.open(self.realpath, flags, *mode), flag2mode(flags))
            self.fd = self.file.fileno()
            self.direct_io = False
            self.keep_cache = True

        def read(self, length, offset):
            self.file.seek(offset)
            return self.file.read(length)

        def release(self, flags):
            self.file.close()
                                                                                                

def main():

    usage = """
BaculaFS: expose the Bacula catalog as a user-space file system

""" + Fuse.fusage

    bacula_version = bextract_version()
    if not bacula_version :
        raise fuse.FuseError('cannot determine Bacula bextract version - is it installed?')
    
    server = FileSystem(version="BaculaFS version: %s\nbextract version: %s\nPython FUSE version: %s" % (__version__, bacula_version, fuse.__version__), usage=usage)

    server.multithreaded = False

    server.parser.add_option(mountopt="driver", metavar="DRIVER", default=server.driver,
                             help="database driver [default: %default]")
    server.parser.add_option(mountopt="address", metavar="SERVER", default=server.address,
                             help="database server address [default: %default]")
    server.parser.add_option(mountopt="port", metavar="PORT", default=server.port, type="int",
                             help="database server port")
    server.parser.add_option(mountopt="database", metavar="PATH", default=server.database,
                             help="database name [default: %default]")
    server.parser.add_option(mountopt="username", metavar="USERNAME", default=server.username,
                             help="database user name [default: %default]")
    server.parser.add_option(mountopt="password", metavar="PASSWORD", default=server.password,
                             help="database password")
    server.parser.add_option(mountopt="conf", metavar="PATH", default=server.conf,
                             help="storage daemon configuration file [default: %default]")
    server.parser.add_option(mountopt="client", metavar="CLIENT", default=server.client,
                             help="file daemon name")
    server.parser.add_option(mountopt="fileset", metavar="FILESET", default=server.fileset,
                             help="backup fileset")
    server.parser.add_option(mountopt="device", metavar="DEVICE", default=server.device,
                             help="storage device name [default: %default]")
    server.parser.add_option(mountopt="datetime", metavar="'YYYY-MM-DD hh:mm:ss'", default=server.datetime,
                             help="snapshot date/time [default: now]")
    server.parser.add_option(mountopt="recent_job", action="store_true", default=server.recent_job,
                             help="select contents of most recent job only [default: %default]") 
    server.parser.add_option(mountopt="move_root", action="store_true", default=server.move_root,
                             help="make absolute path symlinks point to path under mount point  [default: %default]")
    server.parser.add_option(mountopt="prefetch_attrs", action="store_true", default=server.prefetch_symlinks,
                             help="read and parse attributes for all files upon filesystem initialization  [default: %default]")
    server.parser.add_option(mountopt="prefetch_symlinks", action="store_true", default=server.prefetch_symlinks,
                             help="extract all symbolic links upon filesystem initialization (implies prefetch_attrs) [default: %default]")
    server.parser.add_option(mountopt="prefetch_regex", metavar="REGEX", default=server.prefetch_regex,
                             help="extract all objects that match REGEX upon filesystem initialization (implies prefetch_attrs) [default: %default]")
    server.parser.add_option(mountopt="prefetch_recent", action="store_true", default=server.prefetch_recent,
                             help="extract contents of most recent non-full job upon filesystem initialization (implies prefetch_symlinks) [default: %default]")
    server.parser.add_option(mountopt="prefetch_everything", action="store_true", default=server.prefetch_everything,
                             help="extract everything upon filesystem initialization (complete restore to cache) [default: %default]")
    server.parser.add_option(mountopt="retry_interval", metavar="SECONDS", default=server.retry_interval, type="int",
                             help="time interval between retries in case of storage volume access failure [default: %default]")
    server.parser.add_option(mountopt="user_cache_path", metavar="PATH", default=server.user_cache_path,
                             help="user specified cache path (hint: combine this with one of the prefetch options) [default: %default]")
    server.parser.add_option(mountopt="notify", action="store_true", default=server.notify,
                             help="enable notifications [default: %default]")
    server.parser.add_option(mountopt="trace", action="store_true", default=server.trace,
                             help="dump debug trace to console (should be used with -f option) [default: %default]")

    server.parse(values=server, errex=1)

    server.initialize()
    
    server.main()

        
if __name__ == '__main__':
    sys.exit(main())
